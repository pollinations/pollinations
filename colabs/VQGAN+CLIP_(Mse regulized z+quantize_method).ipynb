{"nbformat": 4, "nbformat_minor": 0, "metadata": {"accelerator": "GPU", "colab": {"name": "Mse regulized VQGANCLIP_zquantize public.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"name": "python"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "CppIQlPhhwhs"}, "source": ["# Generates images from text prompts with VQGAN and CLIP (z+quantize method).\n", "\n", "By jbustter https://twitter.com/jbusted1 .\n", "Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n"]}, {"cell_type": "code", "metadata": {"cellView": "form", "id": "pZmCmyKM9fmv", "tags": ["parameters"]}, "source": ["text_input = 'a photo-realistic and beautiful painting of an old man sitting on a chair next to a giant iceberg and looking at a green lush landscape.'  #@param {type: \"string\"}\n", "output_path = \"/content/output\""], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "MI7dufJKpXQ_", "cellView": "form"}, "source": ["#@title Upscale images/video frames\n", "\n", "import os.path as osp\n", "import glob\n", "import cv2\n", "import numpy as np\n", "import torch\n", "\n", "import requests\n", "import imageio\n", "import requests\n", "import warnings\n", "import gdown\n", "\n", "\n", "loaded_upscale_model = False\n", "upscale_device = None\n", "upscale_model = None\n", "def upscale(path):\n", "  global loaded_upscale_model, upscale_device, upscale_model\n", "  \n", "  if not loaded_upscale_model:\n", "\n", "    print(\"Loading superresolution model\")\n", "\n", "    !git clone https://github.com/xinntao/ESRGAN\n", "    %cd ESRGAN\n", "    import RRDBNet_arch as arch\n", "    print(\"Downloading Super-Resolution model\")\n", "    output1 = 'RRDB_ESRGAN_x4.pth'\n", "    print ('Downloading RRDB_ESRGAN_x4.pth')\n", "    gdown.download('https://drive.google.com/uc?id=1TPrz5QKd8DHHt1k8SRtm6tMiPjz_Qene', output1,quiet=False) \n", "\n", "\n", "    warnings.filterwarnings(\"ignore\")\n", "\n", "    Choose_device = \"cuda\" \n", "    model_path = 'RRDB_ESRGAN_x4.pth'\n", "\n", "    upscale_device = torch.device(Choose_device) \n", "\n", "\n", "    upscale_model = arch.RRDBNet(3, 3, 64, 23, gc=32)\n", "    upscale_model.load_state_dict(torch.load(model_path), strict=True)\n", "    upscale_model.eval()\n", "    upscale_model = upscale_model.to(upscale_device)\n", "\n", "    print('Model path {:s}. \\nTesting...'.format(model_path))\n", "    \n", "    %cd -\n", "    loaded_upscale_model = True\n", "  img = cv2.imread(path, cv2.IMREAD_COLOR)\n", "  img = img * 1.0 / 255\n", "  img = torch.from_numpy(np.transpose(img[:, :, [2, 1, 0]], (2, 0, 1))).float()\n", "  img_LR = img.unsqueeze(0)\n", "  img_LR = img_LR.to(upscale_device)\n", "\n", "  print(\"4x upscaling\", path)\n", "  with torch.no_grad():\n", "      output = upscale_model(img_LR).data.squeeze().float().cpu().clamp_(0, 1).numpy()\n", "  \n", "  output = np.transpose(output[[2, 1, 0], :, :], (1, 2, 0))\n", "  output = (output * 255.0).round()\n", "  cv2.imwrite(path, output, [int(cv2.IMWRITE_JPEG_QUALITY), 70])\n", "  print(\"Done upscaling\")\n", "\n", "#import os\n", "\n", " \n", "# iterate over files in\n", "# that directory\n", "#for file in glob.glob(f\"{output_path}/*.jpg\"):\n", "#    print(\"upscaling\", file)\n", "#    upscale(file)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "wSfISAhyPmyp"}, "source": ["!mkdir -p $output_path\n", "\n", "!git clone https://github.com/openai/CLIP\n", "!git clone https://github.com/CompVis/taming-transformers\n", "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n", "!pip install kornia\n", "!pip install einops"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "FhhdWrSxQhwg"}, "source": ["from pathlib import Path\n", "\n", "if not Path(\"vqgan_imagenet_f16_16384.yaml\").exists():\n", "    !wget -N 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' -O vqgan_imagenet_f16_16384.yaml\n", "    !wget -N 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' -O vqgan_imagenet_f16_16384.ckpt"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "EXMSuW2EQWsd"}, "source": ["import argparse\n", "import math\n", "from pathlib import Path\n", "import sys\n", "\n", "sys.path.append('./taming-transformers')\n", "\n", "from IPython import display\n", "from omegaconf import OmegaConf\n", "from PIL import Image\n", "from taming.models import cond_transformer, vqgan\n", "import torch\n", "from torch import nn, optim\n", "from torch.nn import functional as F\n", "from torchvision import transforms\n", "from torchvision.transforms import functional as TF\n", "from tqdm.notebook import tqdm\n", "import numpy as np\n", "\n", "from CLIP import clip\n", "\n", "import kornia.augmentation as K"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "JvnTBhPGT1gn"}, "source": ["def noise_gen(shape):\n", "    n, c, h, w = shape\n", "    noise = torch.zeros([n, c, 1, 1])\n", "    for i in reversed(range(5)):\n", "        h_cur, w_cur = h // 2**i, w // 2**i\n", "        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n", "        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n", "    return noise\n", "\n", "\n", "def sinc(x):\n", "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n", "\n", "\n", "def lanczos(x, a):\n", "    cond = torch.logical_and(-a < x, x < a)\n", "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n", "    return out / out.sum()\n", "\n", "\n", "def ramp(ratio, width):\n", "    n = math.ceil(width / ratio + 1)\n", "    out = torch.empty([n])\n", "    cur = 0\n", "    for i in range(out.shape[0]):\n", "        out[i] = cur\n", "        cur += ratio\n", "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n", "\n", "\n", "def resample(input, size, align_corners=True):\n", "    n, c, h, w = input.shape\n", "    dh, dw = size\n", "\n", "    input = input.view([n * c, 1, h, w])\n", "\n", "    if dh < h:\n", "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n", "        pad_h = (kernel_h.shape[0] - 1) // 2\n", "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n", "        input = F.conv2d(input, kernel_h[None, None, :, None])\n", "\n", "    if dw < w:\n", "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n", "        pad_w = (kernel_w.shape[0] - 1) // 2\n", "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n", "        input = F.conv2d(input, kernel_w[None, None, None, :])\n", "\n", "    input = input.view([n, c, h, w])\n", "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n", "    \n", "\n", "# def replace_grad(fake, real):\n", "#     return fake.detach() - real.detach() + real\n", "\n", "\n", "class ReplaceGrad(torch.autograd.Function):\n", "    @staticmethod\n", "    def forward(ctx, x_forward, x_backward):\n", "        ctx.shape = x_backward.shape\n", "        return x_forward\n", "\n", "    @staticmethod\n", "    def backward(ctx, grad_in):\n", "        return None, grad_in.sum_to_size(ctx.shape)\n", "\n", "\n", "class ClampWithGrad(torch.autograd.Function):\n", "    @staticmethod\n", "    def forward(ctx, input, min, max):\n", "        ctx.min = min\n", "        ctx.max = max\n", "        ctx.save_for_backward(input)\n", "        return input.clamp(min, max)\n", "\n", "    @staticmethod\n", "    def backward(ctx, grad_in):\n", "        input, = ctx.saved_tensors\n", "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n", "\n", "replace_grad = ReplaceGrad.apply\n", "\n", "clamp_with_grad = ClampWithGrad.apply\n", "# clamp_with_grad = torch.clamp\n", "\n", "def vector_quantize(x, codebook):\n", "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n", "    indices = d.argmin(-1)\n", "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n", "    return replace_grad(x_q, x)\n", "\n", "\n", "class Prompt(nn.Module):\n", "    def __init__(self, embed, weight=1., stop=float('-inf')):\n", "        super().__init__()\n", "        self.register_buffer('embed', embed)\n", "        self.register_buffer('weight', torch.as_tensor(weight))\n", "        self.register_buffer('stop', torch.as_tensor(stop))\n", "\n", "    def forward(self, input):\n", "        \n", "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n", "        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)\n", "\n", "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n", "        dists = dists * self.weight.sign()\n", "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n", "\n", "\n", "def parse_prompt(prompt):\n", "    vals = prompt.rsplit(':', 2)\n", "    vals = vals + ['', '1', '-inf'][len(vals):]\n", "    return vals[0], float(vals[1]), float(vals[2])\n", "\n", "def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n", "    input_normed = F.normalize(input, dim=-1)\n", "    target_normed = F.normalize(target, dim=-1)\n", "    logits = input_normed @ target_normed.T * logit_scale\n", "    if labels is None:\n", "        labels = torch.arange(len(input), device=logits.device)\n", "    return F.cross_entropy(logits, labels)\n", "\n", "class MakeCutouts(nn.Module):\n", "    def __init__(self, cut_size, cutn, cut_pow=1.):\n", "        super().__init__()\n", "        self.cut_size = cut_size\n", "        self.cutn = cutn\n", "        self.cut_pow = cut_pow\n", "\n", "        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n", "        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n", "\n", "    def set_cut_pow(self, cut_pow):\n", "      self.cut_pow = cut_pow\n", "\n", "    def forward(self, input):\n", "        sideY, sideX = input.shape[2:4]\n", "        max_size = min(sideX, sideY)\n", "        min_size = min(sideX, sideY, self.cut_size)\n", "        cutouts = []\n", "        cutouts_full = []\n", "        \n", "        min_size_width = min(sideX, sideY)\n", "        lower_bound = float(self.cut_size/min_size_width)\n", "        \n", "        for ii in range(self.cutn):\n", "            \n", "            \n", "          size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n", "\n", "          offsetx = torch.randint(0, sideX - size + 1, ())\n", "          offsety = torch.randint(0, sideY - size + 1, ())\n", "          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n", "          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n", "\n", "        \n", "        cutouts = torch.cat(cutouts, dim=0)\n", "\n", "        if args.use_augs:\n", "          cutouts = augs(cutouts)\n", "\n", "        if args.noise_fac:\n", "          facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n", "          cutouts = cutouts + facs * torch.randn_like(cutouts)\n", "        \n", "\n", "        return clamp_with_grad(cutouts, 0, 1)\n", "\n", "\n", "def load_vqgan_model(config_path, checkpoint_path):\n", "    config = OmegaConf.load(config_path)\n", "    if config.model.target == 'taming.models.vqgan.VQModel':\n", "        model = vqgan.VQModel(**config.model.params)\n", "        model.eval().requires_grad_(False)\n", "        model.init_from_ckpt(checkpoint_path)\n", "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n", "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n", "        parent_model.eval().requires_grad_(False)\n", "        parent_model.init_from_ckpt(checkpoint_path)\n", "        model = parent_model.first_stage_model\n", "    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n", "        model = vqgan.GumbelVQ(**config.model.params)\n", "        model.eval().requires_grad_(False)\n", "        model.init_from_ckpt(checkpoint_path)\n", "    else:\n", "        raise ValueError(f'unknown model type: {config.model.target}')\n", "    del model.loss\n", "    return model\n", "\n", "def resize_image(image, out_size):\n", "    ratio = image.size[0] / image.size[1]\n", "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n", "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n", "    return image.resize(size, Image.LANCZOS)\n", "\n", "class TVLoss(nn.Module):\n", "    def forward(self, input):\n", "        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n", "        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n", "        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n", "        diff = x_diff**2 + y_diff**2 + 1e-8\n", "        return diff.mean(dim=1).sqrt().mean()\n", "\n", "class GaussianBlur2d(nn.Module):\n", "    def __init__(self, sigma, window=0, mode='reflect', value=0):\n", "        super().__init__()\n", "        self.mode = mode\n", "        self.value = value\n", "        if not window:\n", "            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n", "        if sigma:\n", "            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n", "            kernel /= kernel.sum()\n", "        else:\n", "            kernel = torch.ones([1])\n", "        self.register_buffer('kernel', kernel)\n", "\n", "    def forward(self, input):\n", "        n, c, h, w = input.shape\n", "        input = input.view([n * c, 1, h, w])\n", "        start_pad = (self.kernel.shape[0] - 1) // 2\n", "        end_pad = self.kernel.shape[0] // 2\n", "        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n", "        input = F.conv2d(input, self.kernel[None, None, None, :])\n", "        input = F.conv2d(input, self.kernel[None, None, :, None])\n", "        return input.view([n, c, h, w])\n", "\n", "class EMATensor(nn.Module):\n", "    \"\"\"implmeneted by Katherine Crowson\"\"\"\n", "    def __init__(self, tensor, decay):\n", "        super().__init__()\n", "        self.tensor = nn.Parameter(tensor)\n", "        self.register_buffer('biased', torch.zeros_like(tensor))\n", "        self.register_buffer('average', torch.zeros_like(tensor))\n", "        self.decay = decay\n", "        self.register_buffer('accum', torch.tensor(1.))\n", "        self.update()\n", "    \n", "    @torch.no_grad()\n", "    def update(self):\n", "        if not self.training:\n", "            raise RuntimeError('update() should only be called during training')\n", "\n", "        self.accum *= self.decay\n", "        self.biased.mul_(self.decay)\n", "        self.biased.add_((1 - self.decay) * self.tensor)\n", "        self.average.copy_(self.biased)\n", "        self.average.div_(1 - self.accum)\n", "\n", "    def forward(self):\n", "        if self.training:\n", "            return self.tensor\n", "        return self.average\n", "\n", "%mkdir /content/vids"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "WN4OtaLbHBN6"}, "source": ["# ARGS"]}, {"cell_type": "code", "metadata": {"id": "tLw9p5Rzacso"}, "source": ["args = argparse.Namespace(\n", "    \n", "    prompts=[text_input],\n", "    size=[384, 256], \n", "    init_image= None,\n", "    init_weight= 0.5,\n", "\n", "    # clip model settings\n", "    clip_model='ViT-B/32',\n", "    vqgan_config='vqgan_imagenet_f16_16384.yaml',         \n", "    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt',\n", "    step_size=0.1,\n", "    \n", "    # cutouts / crops\n", "    cutn=32,\n", "    cut_pow=1,\n", "    cut_size=100,\n", "\n", "    # display\n", "    display_freq=5,\n", "    seed=None,\n", "    use_augs = True,\n", "    noise_fac= 0.1,\n", "\n", "    record_generation=True,\n", "\n", "    # noise and other constraints\n", "    use_noise = None,\n", "    constraint_regions = False,#\n", "    \n", "    \n", "    # add noise to embedding\n", "    noise_prompt_weights = None,\n", "    noise_prompt_seeds = [14575],#\n", "\n", "    # mse settings\n", "    mse_withzeros = True,\n", "    mse_decay_rate = 50,\n", "    mse_epoches = 5,\n", "\n", "    # end itteration\n", "    max_itter = 1000,\n", ")\n", "\n", "mse_decay = 0\n", "if args.init_weight:\n", "  mse_decay = args.init_weight / args.mse_epoches\n", "\n", "# <AUGMENTATIONS>\n", "augs = nn.Sequential(\n", "    \n", "    K.RandomHorizontalFlip(p=0.5),\n", "    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n", "    K.RandomPerspective(0.2,p=0.4, ),\n", "    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n", "\n", "    )\n", "\n", "noise = noise_gen([1, 3, args.size[0], args.size[1]])\n", "image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n", "image.save('init3.png')"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QXgTa_JWi7Sn"}, "source": ["### Actually do the run..."]}, {"cell_type": "code", "metadata": {"id": "g7EDme5RYCrt"}, "source": ["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n", "\n", "print('Using device:', device)\n", "print('using prompts: ', args.prompts)\n", "\n", "tv_loss = TVLoss() \n", "\n", "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n", "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n", "mse_weight = args.init_weight\n", "\n", "cut_size = args.cut_size\n", "# e_dim = model.quantize.e_dim\n", "\n", "if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n", "    e_dim = 256\n", "    n_toks = model.quantize.n_embed\n", "    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n", "    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n", "else:\n", "    e_dim = model.quantize.e_dim\n", "    n_toks = model.quantize.n_e\n", "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n", "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n", "\n", "\n", "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n", "\n", "f = 2**(model.decoder.num_resolutions - 1)\n", "toksX, toksY = args.size[0] // f, args.size[1] // f\n", "\n", "if args.seed is not None:\n", "    torch.manual_seed(args.seed)\n", "\n", "if args.init_image:\n", "    pil_image = Image.open(args.init_image).convert('RGB')\n", "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n", "    pil_image = TF.to_tensor(pil_image)\n", "    if args.use_noise:\n", "      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n", "    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n", "\n", "else:\n", "    \n", "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n", "\n", "    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n", "        z = one_hot @ model.quantize.embed.weight\n", "    else:\n", "        z = one_hot @ model.quantize.embedding.weight\n", "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n", "\n", "if args.mse_withzeros and not args.init_image:\n", "  z_orig = torch.zeros_like(z)\n", "else:\n", "  z_orig = z.clone()\n", "\n", "z.requires_grad = True\n", "\n", "opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n", "\n", "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n", "                                 std=[0.26862954, 0.26130258, 0.27577711])\n", "\n", "pMs = []\n", "\n", "if args.noise_prompt_weights and args.noise_prompt_seeds:\n", "  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n", "    gen = torch.Generator().manual_seed(seed)\n", "    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n", "    pMs.append(Prompt(embed, weight).to(device))\n", "\n", "for prompt in args.prompts:\n", "    txt, weight, stop = parse_prompt(prompt)\n", "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n", "    pMs.append(Prompt(embed, weight, stop).to(device))\n", "\n", "\n", "def synth(z, quantize=True):\n", "    if args.constraint_regions:\n", "      z = replace_grad(z, z * z_mask)\n", "\n", "    if quantize:\n", "      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n", "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n", "      else:\n", "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n", "\n", "    else:\n", "      z_q = z.model\n", "\n", "    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n", "\n", "@torch.no_grad()\n", "def checkin(i, losses):\n", "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n", "    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n", "    out = synth(z, True)\n", "\n", "    TF.to_pil_image(out[0].cpu()).save(f'progress.png')   \n", "    display.display(display.Image('progress.png')) \n", "\n", "\n", "def ascend_txt():\n", "    global mse_weight\n", "\n", "    out = synth(z)\n", "\n", "    if args.record_generation:\n", "      with torch.no_grad():\n", "        global vid_index\n", "        out_a = synth(z, True)\n", "        if vid_index % 5 == 0:\n", "          filename = f'{output_path}/progress_{vid_index:05}.jpg'\n", "          TF.to_pil_image(out_a[0].cpu()).save(filename)\n", "          upscale(filename)\n", "        vid_index += 1\n", "\n", "    cutouts = make_cutouts(out)\n", "    cutouts = resample(cutouts, (perceptor.visual.input_resolution, perceptor.visual.input_resolution))\n", "\n", "    iii = perceptor.encode_image(normalize(cutouts)).float()\n", "\n", "    result = []\n", "\n", "    if args.init_weight:\n", "        \n", "        global z_orig\n", "        \n", "        result.append(F.mse_loss(z, z_orig) * mse_weight / 2)\n", "        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n", "\n", "        with torch.no_grad():\n", "          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n", "\n", "            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n", "              mse_weight = mse_weight - mse_decay\n", "              print(f\"updated mse weight: {mse_weight}\")\n", "            else:\n", "              mse_weight = 0\n", "              print(f\"updated mse weight: {mse_weight}\")\n", "\n", "    for prompt in pMs:\n", "        result.append(prompt(iii))\n", "\n", "    return result\n", "\n", "vid_index = 0\n", "def train(i):\n", "    \n", "    opt.zero_grad()\n", "    lossAll = ascend_txt()\n", "\n", "    if i % args.display_freq == 0:\n", "        checkin(i, lossAll)\n", "    \n", "    loss = sum(lossAll)\n", "\n", "    loss.backward()\n", "    opt.step()\n", "\n", "i = 0\n", "try:\n", "    with tqdm() as pbar:\n", "        while True and i != args.max_itter:\n", "\n", "            train(i)\n", "\n", "            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n", "              \n", "              opt = optim.Adam([z], lr=args.step_size, weight_decay=0.00000000)\n", "\n", "            i += 1\n", "            pbar.update()\n", "\n", "except KeyboardInterrupt:\n", "    pass\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CDUaCaRnUKMZ"}, "source": ["# create video"]}, {"cell_type": "code", "metadata": {"id": "DT3hKb5gJUPq"}, "source": ["out_file=output_path+\"/video.mp4\"\n", "#!rm /content/*.mp4\n", "last_frame=!ls -t $output_path/*.jpg | head -1\n", "last_frame = last_frame[0]\n", "!cp -v $last_frame $output_path/0000.jpg\n", "!ffmpeg  -r 10 -i $output_path/%*.jpg -y -c:v libx264 -profile:v high -level:v 4.0 /tmp/vid_no_audio.mp4\n", "!ffmpeg -i /tmp/vid_no_audio.mp4 -f lavfi -i anullsrc -c:v copy -c:a aac -shortest -y \"$out_file\"\n", "#ffmpeg -i input.mp4 -f lavfi -c:v copy -c:a aac -shortest output.mp4\n", "#!cp -v /tmp/video.mp4 \"$out_file\"\n", "#!rm /content/taming-transformers/*.png\n", "print(\"Written\", out_file)\n", "!sleep 2"], "execution_count": null, "outputs": []}]}