{
  "request": {
    "url": "http://ec2-3-80-56-235.compute-1.amazonaws.com:16385/openai",
    "method": "POST",
    "body": {
      "type": "json",
      "data": {
        "model": "openai",
        "messages": [
          {
            "role": "user",
            "content": "Different message 1 - 1768340874358"
          }
        ]
      }
    },
    "params": {}
  },
  "response": {
    "status": 200,
    "statusText": "OK",
    "headers": {
      "access-control-allow-origin": "*",
      "connection": "keep-alive",
      "content-length": "1944",
      "content-type": "application/json; charset=utf-8",
      "date": "Tue, 13 Jan 2026 21:48:14 GMT",
      "etag": "W/\"798-QraprgY0bo3XtEBY3lAmFeI9ujs\"",
      "keep-alive": "timeout=5",
      "x-model-used": "gpt-5-mini-2025-08-07",
      "x-powered-by": "Express",
      "x-usage-completion-reasoning-tokens": "1984",
      "x-usage-completion-text-tokens": "235",
      "x-usage-prompt-text-tokens": "258",
      "x-usage-total-tokens": "2477"
    },
    "body": {
      "type": "json",
      "data": {
        "choices": [
          {
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "protected_material_code": {
                "filtered": false,
                "detected": false
              },
              "protected_material_text": {
                "filtered": false,
                "detected": false
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            },
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
              "annotations": [],
              "content": "Do you want me to do something with that string — decode the number, rephrase the whole line, make alternate messages, or something else?\n\nQuick useful guess: if 1768340874358 is a Unix timestamp in milliseconds, it corresponds to 2026-01-13 21:47:54.358 UTC. I can convert that to your local timezone if you tell me where you are.\n\nSome ready-made variations (pick one or ask for another style):\n\n- Short: Different message 1 — 2026-01-13 21:47:54 UTC\n- Formal: Message 1 (timestamp: 1768340874358 ms) — 2026-01-13T21:47:54.358Z\n- Playful: Different message #1 — arrived 2026-01-13 at 21:47:54 UTC\n- Log-friendly: MSG1|1768340874358|2026-01-13T21:47:54.358Z\n\nWhich of those helps, or what would you like me to do next?",
              "refusal": null,
              "role": "assistant"
            }
          }
        ],
        "created": 1768340874,
        "id": "pllns_9f9e558fbac0c1828f9cc9188edabd56",
        "model": "gpt-5-mini-2025-08-07",
        "object": "chat.completion",
        "prompt_filter_results": [
          {
            "prompt_index": 0,
            "content_filter_results": {
              "hate": {
                "filtered": false,
                "severity": "safe"
              },
              "jailbreak": {
                "filtered": false,
                "detected": false
              },
              "self_harm": {
                "filtered": false,
                "severity": "safe"
              },
              "sexual": {
                "filtered": false,
                "severity": "safe"
              },
              "violence": {
                "filtered": false,
                "severity": "safe"
              }
            }
          }
        ],
        "system_fingerprint": null,
        "usage": {
          "completion_tokens": 2219,
          "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 1984,
            "rejected_prediction_tokens": 0
          },
          "prompt_tokens": 258,
          "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
          },
          "total_tokens": 2477
        }
      }
    }
  }
}