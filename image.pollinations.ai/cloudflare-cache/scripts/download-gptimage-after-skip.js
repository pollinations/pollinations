#!/usr/bin/env node

/**
 * Simple script to skip 1 million entries and download images with model_gptimage
 */

import fs from "fs";
import path from "path";

// Configuration
const WORKER_URL = "https://temp-r2-explorer.thomash-efd.workers.dev";
const SKIP_COUNT = 1000000; // Skip first million entries
const SKIP_BATCH_SIZE = 10000; // Skip in smaller batches to avoid timeouts
const MAX_DOWNLOADS = 50;
const OUTPUT_DIR = "./downloads";

async function main() {
    console.log("üöÄ Starting download script...");
    console.log(`üìç Skipping first ${SKIP_COUNT.toLocaleString()} entries`);
    console.log(`üéØ Looking for objects with 'model_gptimage' in key`);
    console.log(`üìä Max downloads: ${MAX_DOWNLOADS}`);

    // Create output directory
    if (!fs.existsSync(OUTPUT_DIR)) {
        fs.mkdirSync(OUTPUT_DIR, { recursive: true });
        console.log(`üìÅ Created directory: ${OUTPUT_DIR}`);
    }

    try {
        // Step 1: Skip the first million entries in batches
        console.log(
            `\n‚è≠Ô∏è  Skipping ${SKIP_COUNT.toLocaleString()} objects in batches of ${SKIP_BATCH_SIZE.toLocaleString()}...`,
        );

        let totalSkipped = 0;
        let cursor = null;

        while (totalSkipped < SKIP_COUNT) {
            const remainingToSkip = SKIP_COUNT - totalSkipped;
            const currentBatchSize = Math.min(SKIP_BATCH_SIZE, remainingToSkip);

            console.log(
                `‚è≠Ô∏è  Skipping batch: ${currentBatchSize.toLocaleString()} objects (total: ${totalSkipped.toLocaleString()})...`,
            );

            const skipUrl = `${WORKER_URL}?action=skip&skipCount=${currentBatchSize}${cursor ? `&cursor=${cursor}` : ""}`;
            const skipResponse = await fetch(skipUrl);

            if (!skipResponse.ok) {
                throw new Error(
                    `Skip request failed: ${skipResponse.status} ${skipResponse.statusText}`,
                );
            }

            const skipData = await skipResponse.json();

            if (!skipData || typeof skipData.skipped === "undefined") {
                throw new Error(
                    `Invalid skip response: ${JSON.stringify(skipData)}`,
                );
            }

            totalSkipped += skipData.skipped;
            cursor = skipData.cursor;

            if (skipData.skipped < currentBatchSize) {
                console.log(
                    `‚ö†Ô∏è  Reached end of bucket at ${totalSkipped.toLocaleString()} objects`,
                );
                break;
            }
        }

        console.log(
            `‚úÖ Total skipped: ${totalSkipped.toLocaleString()} objects`,
        );

        // Now get objects from this cursor position
        const finalListResponse = await fetch(
            `${WORKER_URL}?action=list&limit=100${cursor ? `&cursor=${cursor}` : ""}`,
        );
        if (!finalListResponse.ok) {
            throw new Error(
                `Final list request failed: ${finalListResponse.status} ${finalListResponse.statusText}`,
            );
        }

        const finalListData = await finalListResponse.json();
        if (!finalListData || !finalListData.objects) {
            throw new Error(`Invalid final list response`);
        }

        console.log(
            `üîç Found ${finalListData.objects.length} objects after skip`,
        );

        // Update cursor for the main loop
        cursor = finalListData.cursor;

        // Step 2: Filter for gptimage objects and download
        let downloadCount = 0;

        while (downloadCount < MAX_DOWNLOADS && cursor) {
            console.log(`\nüìã Fetching batch starting from cursor...`);

            // Get a batch of objects
            const listResponse = await fetch(
                `${WORKER_URL}?action=list&limit=100&cursor=${cursor}`,
            );

            if (!listResponse.ok) {
                console.log(
                    `‚ùå List request failed: ${listResponse.status} ${listResponse.statusText}`,
                );
                break;
            }

            const listData = await listResponse.json();

            if (!listData || !listData.objects) {
                console.log(
                    `‚ùå Invalid list response: ${JSON.stringify(listData)}`,
                );
                break;
            }

            console.log(`üì¶ Got ${listData.objects.length} objects in batch`);

            // Filter for gptimage objects
            const gptimageObjects = listData.objects.filter((obj) =>
                obj.key.includes("model_gptimage"),
            );

            console.log(
                `üéØ Found ${gptimageObjects.length} gptimage objects in this batch`,
            );

            // Download each gptimage object
            for (const obj of gptimageObjects) {
                if (downloadCount >= MAX_DOWNLOADS) break;

                console.log(
                    `\nüì• Downloading ${downloadCount + 1}/${MAX_DOWNLOADS}: ${obj.key.substring(0, 80)}...`,
                );

                try {
                    // Get the object data (our worker returns the object as JSON)
                    const downloadResponse = await fetch(
                        `${WORKER_URL}?action=download&key=${encodeURIComponent(obj.key)}`,
                    );

                    if (!downloadResponse.ok) {
                        console.log(
                            `‚ùå Failed to download: ${downloadResponse.status}`,
                        );
                        continue;
                    }

                    const objectData = await downloadResponse.json();

                    // The object body is in objectData.body (as ArrayBuffer)
                    if (!objectData.body) {
                        console.log(`‚ùå No body data in response`);
                        continue;
                    }

                    // Save image - the body comes as base64 or buffer
                    const filename = `gptimage_${downloadCount + 1}_${Date.now()}.jpg`;
                    const filepath = path.join(OUTPUT_DIR, filename);

                    // Convert body to buffer and save
                    let imageBuffer;
                    if (typeof objectData.body === "string") {
                        // If base64 string
                        imageBuffer = Buffer.from(objectData.body, "base64");
                    } else {
                        // If already buffer/array
                        imageBuffer = Buffer.from(objectData.body);
                    }

                    fs.writeFileSync(filepath, imageBuffer);

                    // Save raw object metadata (thin proxy principle)
                    const metadataFilename = `${filename}.json`;
                    const metadataPath = path.join(
                        OUTPUT_DIR,
                        metadataFilename,
                    );
                    fs.writeFileSync(
                        metadataPath,
                        JSON.stringify(objectData, null, 2),
                    );

                    console.log(
                        `‚úÖ Saved: ${filename} (${(obj.size / 1024 / 1024).toFixed(2)} MB)`,
                    );
                    downloadCount++;
                } catch (error) {
                    console.log(
                        `‚ùå Error downloading ${obj.key}: ${error.message}`,
                    );
                }
            }

            // Move to next batch
            if (!listData.truncated || !listData.cursor) {
                console.log(`\nüèÅ Reached end of objects`);
                break;
            }

            cursor = listData.cursor;
        }

        console.log(`\nüéâ Download complete!`);
        console.log(`üìä Downloaded ${downloadCount} images to ${OUTPUT_DIR}`);
    } catch (error) {
        console.error(`‚ùå Error:`, error.message);
        process.exit(1);
    }
}

main().catch(console.error);
