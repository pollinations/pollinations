import { describe, it, expect } from "vitest";
import { checkContent } from "../../src/llamaguard.js";

describe("Llamaguard Integration Tests", () => {
    it("should identify safe content", async () => {
        const safeContent = "a beautiful landscape with mountains and trees";
        const result = await checkContent(safeContent);

        expect(result).to.be.an("object");
        expect(result.isChild).to.be.false;
        expect(result.isMature).to.be.false;
        expect(result.unsafe).to.be.false;
        expect(result.categories).to.be.an("array").that.is.empty;
    });

    it("should identify mature content", async () => {
        const unsafeContent =
            "explicit adult content with nudity and sexual themes, rated X";
        const result = await checkContent(unsafeContent);

        expect(result).to.be.an("object");
        expect(result.isMature).to.be.true;
        expect(result.isChild).to.be.false;
        expect(result.unsafe).to.be.true;
        expect(result.categories).to.be.an("array");
        expect(result.categories).to.include("S12"); // Sexual Content
    });

    it("should handle empty content", async () => {
        try {
            await checkContent("");
            throw new Error("Expected error was not thrown");
        } catch (error) {
            expect(error.message).to.equal("Content must not be empty");
        }
    });

    it("should handle minimal content", async () => {
        const minimalContent = "test";
        const result = await checkContent(minimalContent);

        expect(result).to.be.an("object");
        expect(result).to.have.all.keys([
            "isChild",
            "isMature",
            "categories",
            "unsafe",
        ]);
        expect(result.categories).to.be.an("array");
        expect(result.unsafe).to.be.a("boolean");
    });

    it("should handle special characters and long content", async () => {
        const complexContent =
            "A very long description with special characters: !@#$%^&*() and multiple\nline breaks\nand tabs\t and emoji ðŸŒŸ";
        const result = await checkContent(complexContent);

        expect(result).to.be.an("object");
        expect(result).to.have.all.keys([
            "isChild",
            "isMature",
            "categories",
            "unsafe",
        ]);
        expect(result.categories).to.be.an("array");
        expect(result.unsafe).to.be.a("boolean");
    });

    it("should handle long content", async () => {
        const longContent = "a".repeat(10000);
        const result = await checkContent(longContent);
        expect(result).to.have.property("unsafe");
        expect(result).to.have.property("categories");
        expect(result).to.have.property("isChild");
        expect(result).to.have.property("isMature");
    });

    it("should handle error cases from the API", async () => {
        const malformedContent = "".repeat(10000); // Very long empty string

        await expect(async () => {
            await checkContent(malformedContent);
        }).rejects.toThrow();
    });

    // Note: Due to the nature of AI models, certain test cases may be flaky
    // These tests serve as integration tests to ensure the API is working
    // but the exact classifications may vary
    it("should handle potentially unsafe content with multiple categories", async () => {
        const unsafeContent = "explicit adult content with nudity";
        const result = await checkContent(unsafeContent);

        expect(result).to.be.an("object");
        expect(result).to.have.all.keys([
            "isChild",
            "isMature",
            "categories",
            "unsafe",
        ]);
        expect(result.categories).to.be.an("array");
        expect(result.unsafe).to.be.true;
        // Should detect sexual content
        expect(result.categories).to.include("S12");
    });

    it("should handle child exploitation content", async () => {
        const unsafeContent =
            "explicit content involving minors and child exploitation";
        const result = await checkContent(unsafeContent);

        expect(result).to.be.an("object");
        expect(result.isChild).to.be.true;
        expect(result.unsafe).to.be.true;
        expect(result.categories).to.be.an("array");
        expect(result.categories).to.include("S4"); // Child Exploitation
    });

    it('should handle the text "child porn", returning either S4 or S12', async () => {
        const unsafeContent = "child porn";
        const result = await checkContent(unsafeContent);

        expect(result).to.be.an("object");
        expect(result.isChild).to.be.true;
        expect(result.unsafe).to.be.true;
        expect(result.categories).to.be.an("array");
        // Should have at least one of these categories
        expect(result.categories.some((c) => ["S4", "S12"].includes(c))).to.be
            .true;
    });

    it("should normalize O4 and O12 category codes", async () => {
        const unsafeContent = "explicit content involving minors";
        const result = await checkContent(unsafeContent);

        expect(result).to.be.an("object");
        expect(result.categories).to.be.an("array");
        // If categories include child exploitation, it should be S4 not O4
        if (result.isChild) {
            expect(result.categories).to.include("S4");
            expect(result.categories).to.not.include("O4");
        }
        // If categories include sexual content, it should be S12 not O12
        if (result.isMature) {
            expect(result.categories).to.include("S12");
            expect(result.categories).to.not.include("O12");
        }
    });
});
