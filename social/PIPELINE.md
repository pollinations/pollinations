# Social Media News Pipeline

> **Status:** Implemented â€” 3-tier architecture is the sole active system.
>
> **Assumption:** GitHub is the single source of truth. "Merge to main" is the authoritative event for shipping news.

## Context

The previous system had 9 workflows and 11 scripts where each platform (Twitter, Instagram, LinkedIn, Discord, Reddit) **independently fetched PRs from GitHub and independently analyzed them with AI**. The same PRs got fetched 4-5 times per day, each platform re-interpreted them from scratch, and 3 separate PRs were created daily for review.

The pipeline uses **event-centric interpretation**: each PR is analyzed once at merge time, and all downstream content aggregates from that single analysis.

---

## Architecture: 3 Tiers

```
TIER 1: PER-PR (real-time)
  PR merged â†’ AI analyzes â†’ gist JSON committed to news branch â†’ image generated â†’ Discord post

TIER 2: DAILY (Mon-Sat 06:00 UTC)
  Read day's gists â†’ AI generates daily summary â†’ platform posts (X, IG, Reddit)
  â†’ commit to news branch â†’ Buffer stages X + IG immediately â†’ README-only PR to main
  LinkedIn is weekly-only. Reddit publishes via cron at 15:00 UTC.

TIER 3: WEEKLY (Sunday 06:00 UTC)
  Read week's gists directly (Sunâ†’Sat) â†’ synthesize weekly themes â†’ platform posts (X, IG, LI, Reddit, Discord)
  â†’ commit to news branch â†’ Buffer stages X + LI + IG immediately
  Reddit + Discord publish via cron at 18:00 UTC Sunday.
```

### Branch Strategy

- **`main` branch** â€” source code only. No generated content. README "Latest News" section updated via small automated PRs.
- **`news` branch** â€” all generated content: gists, daily posts, weekly posts, highlights, images. Unprotected (direct commits). Content is reviewed here before cron publishes it.

### Data Flow

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 1: PER-PR (on merge)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PR merge â”€â”€â†’ generate_realtime.py
                â”‚
                â”œâ”€â”€â†’ Step 1: AI analysis â†’ gist JSON (incl. image prompt)
                â”‚    (committed to news branch)
                â”‚
                â””â”€â”€â†’ Step 2: ğŸ¨ GENERATE 1 image (8-bit pixel art)
                     â†’ stored in gist as image.url

           â”€â”€â†’ publish_realtime.py (separate step)
                â””â”€â”€â†’ Reads gist â†’ AI announcement â†’ Discord webhook post

             Images generated: 1 per PR
             Images reused:    Discord reuses gist image

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 2: DAILY (Mon-Sat 06:00 UTC â†’ Buffer immediate â†’ Reddit 15:00 UTC)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

             06:00 UTC â”€â”€â†’ generate_daily.py
                            â”‚  (reads gists, clusters into 3-5 arcs)
                            â”‚
                            â”œâ”€â”€â†’ twitter.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                            â”œâ”€â”€â†’ instagram.json + ğŸ¨ GENERATE 3 images (carousel)
                            â”œâ”€â”€â†’ reddit.json    + ğŸ¨ GENERATE 1 image (brand pixel art)
                            â”œâ”€â”€â†’ highlights.md  (AI curates yesterday's gists)
                            â”‚
                            â”œâ”€â”€â†’ Commit all to news branch
                            â”œâ”€â”€â†’ Buffer stages X + IG immediately (PUBLISH_MODE=buffer)
                            â””â”€â”€â†’ README-only PR to main (Latest News section)
                                  (LinkedIn = weekly only, no daily posts)

             15:00 UTC â”€â”€â†’ NEWS_publish.yml (cron, PUBLISH_MODE=direct)
                            â””â”€â”€â†’ Reddit VPS deployment

             Images generated: 5 (1 twitter + 3 instagram + 1 reddit)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 3: WEEKLY (Sunday 06:00 UTC â†’ Buffer immediate â†’ Reddit+Discord 18:00 UTC)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

             Sunday 06:00 UTC â”€â”€â†’ generate_weekly.py
                                      â”‚  (reads gists directly Sunâ†’Sat,
                                      â”‚   synthesizes weekly themes)
                                      â”‚
                                      â”œâ”€â”€â†’ twitter.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ linkedin.json  + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ instagram.json + ğŸ¨ GENERATE 3 images (carousel)
                                      â”œâ”€â”€â†’ reddit.json    + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ discord.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”‚
                                      â”œâ”€â”€â†’ Commit all to news branch
                                      â””â”€â”€â†’ Buffer stages X + LI + IG immediately (PUBLISH_MODE=buffer)

             Sunday 18:00 UTC â”€â”€â†’ NEWS_publish.yml (cron, PUBLISH_MODE=direct)
                                    â”œâ”€â”€â†’ Reddit VPS deployment
                                    â””â”€â”€â†’ Discord webhook post (with image)

             Images generated: 7 (1 twitter + 1 linkedin + 3 instagram + 1 reddit + 1 discord)
             Images reused:    none
```

---

## Storage

All generated content lives on the **`news` branch**.

### PR Gists: `social/news/gists/YYYY-MM-DD/PR-{number}.json`

- Committed directly to `news` branch (no PR needed â€” small auto-generated JSON)
- One file per merged PR per day
- Unique filenames per PR (`PR-{number}.json`) â€” no git push race conditions
- **Includes pixel art image URL** â€” generated at PR merge time, reused by Discord posts
- Image generation uses our own API â€” retries 3x with exponential backoff + different seed on 5xx errors

```json
{
  "pr_number": 8117,
  "title": "fix(enter): single-bucket balance deduction",
  "author": "username",
  "url": "https://github.com/pollinations/pollinations/pull/8117",
  "merged_at": "2026-02-09T15:30:00Z",
  "labels": ["bug", "enter"],

  "gist": {
    "category": "bug_fix",
    "user_facing": true,
    "publish_tier": "daily",
    "importance": "major",
    "headline": "One bucket to rule them all",
    "blurb": "The bees fixed a leaky honey jar â€” balance deductions now flow through a single bucket instead of spilling across many.",
    "summary": "Fixed balance deduction to use a single bucket instead of splitting across multiple.",
    "impact": "Users no longer see incorrect balance after API calls.",
    "keywords": ["billing", "balance", "api"],
    "visual_concept": "A bee repairing a cracked piggy bank. The piggy bank represents the single-bucket billing system. Wrench = fix.",
    "image_prompt": "Cozy pixel art scene of a tiny bee fixing a cracked piggy bank with a wrench. Soft lime green glow, chunky 8-bit sprites, warm lighting."
  },

  "image": {
    "url": "https://raw.githubusercontent.com/.../PR-8117.jpg",
    "prompt": "Cozy pixel art scene of a tiny bee fixing a cracked piggy bank..."
  },

  "generated_at": "2026-02-09T15:31:00Z"
}
```

**Key fields:**

| Field | Purpose |
|---|---|
| `publish_tier` | `"none"` / `"discord_only"` / `"daily"` â€” controls which tiers pick up this PR. See `publish_tier` decision logic below. |
| `importance` | `"major"` / `"minor"` â€” AI picks. Binary: headline-worthy or not. |
| `user_facing` | Boolean â€” AI determines if end users would notice this change |
| `headline` | Short creative name for the change (3-8 words). Used in daily/weekly narrative arcs. |
| `blurb` | Whimsical 1-2 sentence description for the website diary. Bee/nature metaphors welcome. |
| `visual_concept` | AI reasoning space â€” identifies project mascots, symbols, and scene concept before writing `image_prompt`. |

**Importance is binary:**

- `"major"` â€” headline-worthy. Users would notice or care. Features, significant bug fixes, new models, pricing changes.
- `"minor"` â€” everything else. Chore, deps, infra, small fixes, internal tooling.

The AI picks based on PR content. The daily summary uses `major` PRs as headline arcs; `minor` PRs get mentioned briefly or grouped. No numeric scores, no formula â€” prominence is implicit in the narrative structure, not serialized as extra fields.

**`publish_tier` decision logic:**

The AI chooses `publish_tier` as part of gist analysis, but hard rules act as guardrails:

```
# Hard rules (override AI choice):
if labels include "deps" or "chore" AND user_facing == false:
    publish_tier = "discord_only"       # forced
if labels include "feature":
    publish_tier = min("daily", AI_choice)  # at least "daily"

# AI decides (with default):
if no labels:
    publish_tier = AI_choice            # default: "daily"
else:
    publish_tier = AI_choice            # default: "daily"

# Valid values: "none", "discord_only", "daily"
# ("weekly" is not a valid tier â€” weekly summary reads gists directly with the same "daily" filter)
```

This means: deps/chore PRs can't sneak into daily summaries, features always make it, and everything else the AI decides with a bias toward inclusion.

### Daily Posts: `social/news/daily/YYYY-MM-DD/`

- `twitter.json` â€” platform post JSON
- `instagram.json` â€” same schema
- `reddit.json` â€” same schema (LinkedIn is weekly-only, no daily file)
- `images/` â€” all generated images

### Weekly: `social/news/weekly/YYYY-MM-DD/`

- `twitter.json` â€” weekly recap tweet (1 image)
- `linkedin.json` â€” weekly recap post (1 image)
- `instagram.json` â€” weekly recap carousel (3 images)
- `reddit.json` â€” weekly Reddit post (1 image)
- `discord.json` â€” weekly Discord digest
- `images/` â€” all generated images

---

## Workflows

| Workflow | Trigger | What it does |
|---|---|---|
| `NEWS_pr_gist.yml` | `pull_request_target: closed+merged` | Per-PR gist + Discord |
| `NEWS_summary.yml` | `cron: 0 6 * * *` | Generate content â†’ commit to news â†’ Buffer staging (`PUBLISH_MODE=buffer`) |
| `NEWS_publish.yml` | Mon-Sat 15:00 UTC, Sun 18:00 UTC | Direct channels only (`PUBLISH_MODE=direct`): Reddit daily, Reddit+Discord weekly |

### PUBLISH_MODE

The `PUBLISH_MODE` env var controls which channels fire:

| Mode | Channels | Used by |
|---|---|---|
| `buffer` | Twitter, Instagram (daily), + LinkedIn (weekly) â†’ Buffer queue | `NEWS_summary.yml` (immediately after generation) |
| `direct` | Reddit â†’ VPS, Discord â†’ webhook | `NEWS_publish.yml` (cron) |
| `all` | Both (default) | Manual / testing |

### Workflow overlay

Both `NEWS_summary.yml` and `NEWS_publish.yml` overlay the `news` branch data onto the checkout:

```bash
git fetch origin news && git checkout origin/news -- social/news/
```

This makes all generated content available locally for scripts that read files (e.g. `read_news_file()` in common.py).

## Scripts

| Script | Purpose |
|---|---|
| `generate_realtime.py` | Per-PR: AI analysis â†’ gist JSON â†’ image gen (source of truth) |
| `publish_realtime.py` | Per-PR: reads gist â†’ AI announcement â†’ Discord webhook post |
| `generate_daily.py` | Daily: read gists â†’ summary + platform posts (X, IG, Reddit) + images â†’ commit to news â†’ README-only PR to main |
| `generate_weekly.py` | Weekly: read gists directly (Sunâ†’Sat) â†’ synthesize themes â†’ all 5 platform posts + images â†’ commit to news |
| `publish_daily.py` | PUBLISH_MODE=buffer: stage X + IG to Buffer. PUBLISH_MODE=direct: Reddit VPS deployment. |
| `publish_weekly.py` | PUBLISH_MODE=buffer: stage X + LI + IG to Buffer. PUBLISH_MODE=direct: Reddit VPS + Discord webhook. |
| `update_highlights.py` | Daily: reads yesterday's gists, AI curates highlights, updates highlights.md + README |
| `update_readme.py` | Utility functions: `get_top_highlights()`, `update_readme_news_section()` (used by update_highlights.py) |
| `common.py` | Shared utils: prompt loading, brand injection, API calls, gist I/O, retry logic, `read_news_file()`, constants |
| `buffer_publish.py` | Buffer API staging with scheduled delivery |
| `buffer_utils.py` | Buffer GraphQL API helpers |

---

## Error Handling

### Tier 1: `generate_realtime.py` â€” the critical path

The gist is the anchor for everything downstream. The 2 steps run sequentially:

```
Step 1: AI analysis â†’ gist JSON â†’ validate schema â†’ commit to news branch
  â”œâ”€â”€ Success: proceed to Step 2
  â”œâ”€â”€ Schema validation failure: log warning, commit MINIMAL gist (PR metadata only)
  â”‚   (prevents malformed JSON from reaching downstream tiers)
  â””â”€â”€ AI failure: RETRY up to 3x with exponential backoff
        â””â”€â”€ Still fails: commit a MINIMAL gist (PR metadata only, no AI fields)
            + log error + open GitHub issue tagged "news-pipeline-failure"
            (daily summary sees the PR exists but skips it for narrative)

Step 2: Image generation â†’ update gist with image URL
  â”œâ”€â”€ Success: done (gist fully committed)
  â””â”€â”€ Failure (5xx): RETRY up to 3x with exponential backoff + different seed each attempt
        â””â”€â”€ Still fails: gist.image.url = null, continue without image
            (Discord posts text-only)
```

Discord posting (`publish_realtime.py`) runs as a **separate workflow step** after the gist generator. If Discord posting fails, the gist is already committed (source of truth preserved). Discord is best-effort notification.

### Tier 2: `generate_daily.py`

- If gist directory is **empty** (no PRs merged that day): skip. No posts generated. Quiet days are quiet days.
- If gist directory is **missing** locally: falls back to GitHub API (`?ref=news`) to read gists.

### Tier 3: `generate_weekly.py`

- Reads gists directly for the week (Sunâ†’Sat). No dependency on daily summaries.
- If no daily-tier gists found for the week: skip.

### Re-triggering

All workflows support `workflow_dispatch` for manual re-triggering:
- `NEWS_pr_gist.yml`: accepts `pr_number` input to regenerate a specific gist
- `NEWS_summary.yml`: accepts `date` input (Mon-Sat runs daily, Sunday runs weekly)
- `NEWS_publish.yml`: accepts `mode` + `target_date` for manual publish of direct channels

---

## Concurrency & Race Conditions

### Simultaneous PR merges

Multiple PRs merging within seconds is the main risk for Tier 1.

**Mitigation:** Each gist writes to a **unique filename** (`PR-{number}.json`). There are no file collisions. The GitHub Contents API commit uses the `sha` parameter for conditional writes â€” if two workflows try to create files in the same directory simultaneously, both succeed because they're writing different files. (Unlike editing the same file, creating new files in a directory doesn't conflict.)

### Daily/weekly generators reading while gists are being written

The daily summary runs at 06:00 UTC. A PR merged at 05:59 UTC might have its gist committed at 06:01.

**Mitigation:** The daily summary selects gists by **`merged_at` timestamp**, not by file commit time. It reads all gists where `merged_at` falls on the target date, regardless of when the file appeared. If a gist is committed *after* the daily summary already ran (e.g., slow image gen), it gets picked up by the next day's summary or by the weekly fallback.

---

## Key Design Decisions

1. **PR-time analysis is the anchor** â€” intent is frozen while context is freshest. Eliminates platform drift ("same PR, different story"). Reduces AI cost and variance.

2. **Gists stored as repo files, not GitHub Gist API** â€” auditable, diffable, reviewable. No extra auth surface. Fits existing repo-as-CMS pattern.

3. **All generated content on `news` branch** â€” `main` stays source-code only. Content is reviewable on `news` before publishing. Only the README "Latest News" section touches `main` via automated PR.

4. **`publish_tier` field gates what reaches each tier** â€” non-user-facing PRs default to `discord_only`, preventing "busy weeks" from reading like spam. Daily/weekly layers only consume PRs tagged `daily` or higher.

5. **Importance is binary** â€” `major/minor` chosen by AI. Headline-worthy or not. Prominence is implicit in narrative structure, not serialized as extra fields.

6. **2 sequential steps in Tier 1** â€” gist commit, then image gen. Gist (the anchor) commits first; image gen retries 3x on 5xx (different seed each time). Discord posting is a separate workflow step (`publish_realtime.py`) â€” best-effort, decoupled from the source of truth.

7. **Buffer staging immediately after generation** â€” `NEWS_summary.yml` generates content and immediately stages to Buffer (`PUBLISH_MODE=buffer`). Buffer handles delivery scheduling. Direct channels (Reddit, Discord) use separate cron.

8. **Daily summary clusters related PRs into 3-5 story arcs** â€” 5 PRs about the same subsystem become one narrative beat. Editorial quality, not a changelog.

9. **Three independent image families** â€” see Image Generation Strategy section below.

10. **Highlights + README in the daily flow** â€” `generate_daily.py` curates yesterday's gists into `highlights.md` (committed to news) and updates the README "Latest News" section via a small PR to main.

11. **Weekly delivery at Sunday 18:00 UTC** â€” Sunday evening "week wrap-up" energy. Reddit + Discord via cron. Buffer-staged platforms deliver on Buffer's schedule.

12. **No fallback content for zero-PR days** â€” if no PRs merged, the daily workflow skips entirely. No posts generated. Quiet days are quiet days.

13. **Website diary reads from gists + summary** â€” no separate diary generation step. Gists include `headline` and `blurb` fields; daily/weekly summaries include `mood`. The website can render a diary view directly from these sources.

14. **Weekly reads gists directly, independent of dailies** â€” the weekly summary reads the week's gists (Sunâ†’Sat) and synthesizes themes into a bigger narrative ("this week we shipped X, fixed Y, started Z"). This eliminates the dependency on daily summaries being generated first, ensuring no PRs are missed.

---

## Image Generation Strategy

There are **3 independent families of images**. Each tier generates its own images with its own prompts and style.

| Family | Generated by | When | Style | Count | Used by |
|---|---|---|---|---|---|
| **Per-PR pixel art** | `generate_realtime.py` | Tier 1 (on PR merge) | 8-bit pixel art | 1 per PR | Discord post, website diary |
| **Daily platform images** | `generate_daily.py` | Tier 2 (06:00 UTC) | Brand pixel art (from `brand/visual.md`) | 1 Twitter + 3 Instagram + 1 Reddit = **5 per day** | Twitter, Instagram, Reddit daily posts (LinkedIn = weekly only) |
| **Weekly platform images** | `generate_weekly.py` | Tier 3 (Sunday 06:00 UTC) | Brand pixel art (from `brand/visual.md`) | 1 Twitter + 1 LinkedIn + 3 Instagram + 1 Reddit + 1 Discord = **7 per week** | Twitter, LinkedIn, Instagram, Reddit, Discord weekly posts |

**Key points:**

- **Daily and weekly images are freshly generated** from the daily narrative / weekly summary. They are NOT the per-PR pixel art images. The AI creates images that illustrate the aggregated story, not individual PRs.

---

## Cost Estimate (per day, assuming 5 PRs merged)

| Step | AI calls | Image gens |
|---|---|---|
| PR gists (5x) | 5 | 5 |
| Daily summary (1x) | 2 | 5 (1 twitter + 3 instagram + 1 reddit) |
| **Total** | **7** | **10** |

Weekly adds ~6 AI calls + ~7 image gens on Sundays.

AI calls scale as N+1 (N per-PR gists + 1 daily summary), not NÃ—platforms. Image count stays similar across any architecture (same images needed).

---

## Verification

1. **Tier 1 â€” happy path**: Merge a test PR â†’ verify gist JSON committed to `news` branch `social/news/gists/` + image generated + Discord post sent (separate step)
2. **Tier 1 â€” AI failure**: Mock AI to fail â†’ verify minimal gist (metadata only) committed + GitHub issue opened
3. **Tier 2 â€” happy path**: Manually trigger daily workflow â†’ verify platform posts + images committed to `news` branch + README PR to main
4. **Tier 2 â€” zero PRs**: Run daily workflow on a day with 0 gists â†’ verify workflow exits cleanly with no content generated
5. **Tier 3 â€” happy path**: Manually trigger weekly workflow â†’ verify all 5 platform posts + images committed to `news` branch
6. **Daily publish â€” Buffer**: Verify `NEWS_summary.yml` stages X/IG to Buffer immediately after generation
7. **Daily publish â€” Reddit**: Verify `NEWS_publish.yml` cron at 15:00 UTC deploys Reddit to VPS
8. **Weekly publish â€” Buffer**: Verify `NEWS_summary.yml` stages X/LI/IG to Buffer immediately after generation
9. **Weekly publish â€” Reddit+Discord**: Verify `NEWS_publish.yml` Sunday 18:00 UTC cron publishes Reddit + Discord
10. **Publish tier gating**: Merge a non-user-facing PR â†’ verify `publish_tier: discord_only` â†’ verify absent from daily summary
11. **Clustering**: Day with 5+ related PRs â†’ verify daily summary groups them into narrative arcs (not a flat list)
12. **Concurrent merges**: Merge 3 PRs within 30 seconds â†’ verify all 3 gists committed without conflicts

---

## Critical Files

| File | Role |
|---|---|
| `social/scripts/common.py` | Shared utilities: prompt loading, brand injection, API calls, gist I/O, `read_news_file()`, retry logic, constants |
| `social/scripts/buffer_publish.py` | Buffer API staging with scheduled delivery |
| `social/buffer-schedule.yml` | Delivery schedule for all platforms |

## Prompts

All prompts live in `social/prompts/`:

```
social/prompts/
  brand/                       # Brand components (auto-injected via placeholders)
    about.md                   # Company description       â†’ {about}
    visual.md                  # Pixel art style guide     â†’ {visual_style}
    links.md                   # Official links            â†’ {links}

  tone/                        # Platform voices (system prompts)
    twitter.md                 # Twitter/X voice + image adaptation
    linkedin.md                # LinkedIn voice + image adaptation
    instagram.md               # Instagram voice + image adaptation
    reddit.md                  # Reddit voice + image adaptation
    discord.md                 # Discord voice + image adaptation

  gist.md                      # Tier 1: Analyze PR â†’ gist JSON + image prompt
  daily.md                     # Tier 2: Cluster gists into 3-5 narrative arcs
  weekly.md                    # Tier 3: Synthesize weekly recap from gists
  highlights.md                # Highlights curation for GitHub + README
  format.md                    # Output format specs (JSON schemas per platform)
```

### Brand Injection

`common.py` automatically replaces placeholders in any loaded prompt:

| Placeholder | Source |
|---|---|
| `{about}` | `brand/about.md` |
| `{visual_style}` | `brand/visual.md` |
| `{links}` | `brand/links.md` |

### Prompt Composition Pattern

Every platform post is generated from **three layers** combined:

1. **Brand identity** (`brand/*.md`) â€” injected automatically via placeholders. Defines who we are, visual style, bee mascot.
2. **Platform voice** (`tone/<platform>.md`) â€” system prompt. Defines tone, length, formatting rules, image adaptation for a specific destination.
3. **Output format** (`format.md`) â€” user prompt. Defines the JSON schema and content constraints for each platform.

The AI call structure: `system_prompt = load_prompt("tone/{platform}")` (with brand auto-injected) + `user_prompt = summary_data + load_format("{platform}")`.

This allows reusing the same voice across cadences (daily, weekly) and the same format across content types.
