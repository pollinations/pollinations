# Social Media News Pipeline

> **Status:** Implemented â€” 3-tier architecture is the sole active system.
>
> **Assumption:** GitHub is the single source of truth. "Merge to main" is the authoritative event for shipping news.

## Context

The previous system had 9 workflows and 11 scripts where each platform (Twitter, Instagram, LinkedIn, Discord, Reddit) **independently fetched PRs from GitHub and independently analyzed them with AI**. The same PRs got fetched 4-5 times per day, each platform re-interpreted them from scratch, and 3 separate PRs were created daily for review.

The pipeline uses **event-centric interpretation**: each PR is analyzed once at merge time, and all downstream content aggregates from that single analysis.

---

## Architecture: 3 Tiers

```
TIER 1: PER-PR (real-time)
  PR merged â†’ AI analyzes â†’ gist JSON committed to repo â†’ image generated â†’ Discord post

TIER 2: DAILY (Mon-Sat 06:00 UTC â†’ merge PR â†’ Buffer 15:00 UTC)
  Read day's gists â†’ AI generates daily summary â†’ platform posts (X, IG, Reddit)
  â†’ single PR for review â†’ on merge: Buffer stages X + IG for next 15:00 UTC slot
  LinkedIn is weekly-only. Reddit deployed to VPS via SSH.

TIER 3: WEEKLY (Sunday 06:00 UTC â†’ Sunday 18:00 UTC)
  Read week's gists directly (Sunâ†’Sat) â†’ synthesize weekly themes â†’ platform posts (X, IG, LI, Reddit, Discord)
  Generated Sun 06:00 UTC â†’ PR for review â†’ Sun 18:00 UTC cron publishes all 5 platforms
```

### Data Flow

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 1: PER-PR (on merge)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PR merge â”€â”€â†’ generate_realtime.py
                â”‚
                â”œâ”€â”€â†’ Step 1: AI analysis â†’ gist JSON (incl. image prompt)
                â”‚    (committed to main)
                â”‚
                â””â”€â”€â†’ Step 2: ğŸ¨ GENERATE 1 image (8-bit pixel art)
                     â†’ stored in gist as image.url

           â”€â”€â†’ publish_realtime.py (separate step)
                â””â”€â”€â†’ Reads gist â†’ AI announcement â†’ Discord webhook post

             Images generated: 1 per PR
             Images reused:    Discord reuses gist image

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 2: DAILY (Mon-Sat 06:00 UTC â†’ merge PR â†’ Buffer 15:00 UTC)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

             06:00 UTC â”€â”€â†’ generate_daily.py
                            â”‚  (reads gists, clusters into 3-5 arcs)
                            â”‚
                            â”œâ”€â”€â†’ twitter.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                            â”œâ”€â”€â†’ instagram.json + ğŸ¨ GENERATE 3 images (carousel)
                            â”œâ”€â”€â†’ reddit.json    + ğŸ¨ GENERATE 1 image (brand pixel art)
                            â”œâ”€â”€â†’ highlights.md  (AI curates yesterday's gists)
                            â”œâ”€â”€â†’ README.md      ("Latest News" section update)
                            â”‚
                            â””â”€â”€â†’ Single PR for review
                                  social/news/daily/YYYY-MM-DD/
                                         â”‚ (on merge)
                                         â”œâ”€â”€â†’ Buffer staging (X, IG) at 15:00 UTC
                                         â””â”€â”€â†’ Reddit VPS deployment
                                  (LinkedIn = weekly only, no daily posts)

             Images generated: 5 (1 twitter + 3 instagram + 1 reddit)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 TIER 3: WEEKLY (Sunday 06:00 UTC â†’ Sunday 18:00 UTC)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

             Sunday 06:00 UTC â”€â”€â†’ generate_weekly.py
                                      â”‚  (reads gists directly Sunâ†’Sat,
                                      â”‚   synthesizes weekly themes)
                                      â”‚
                                      â”œâ”€â”€â†’ twitter.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ linkedin.json  + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ instagram.json + ğŸ¨ GENERATE 3 images (carousel)
                                      â”œâ”€â”€â†’ reddit.json    + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â”œâ”€â”€â†’ discord.json   + ğŸ¨ GENERATE 1 image (brand pixel art)
                                      â””â”€â”€â†’ Creates PR for review

             Sunday 18:00 UTC â”€â”€â†’ NEWS_publish.yml (cron)
                                    â”‚ (checks if weekly PR was merged)
                                    â”œâ”€â”€ Not merged â†’ skip
                                    â””â”€â”€ Merged â†’ publish all 5 platforms:
                                          â”œâ”€â”€â†’ Buffer staging (X, LI, IG) at 18:00 UTC
                                          â”œâ”€â”€â†’ Reddit API post
                                          â””â”€â”€â†’ Discord webhook post (with image)

             Images generated: 7 (1 twitter + 1 linkedin + 3 instagram + 1 reddit + 1 discord)
             Images reused:    none
```

---

## Storage

### PR Gists: `social/news/gists/YYYY-MM-DD/PR-{number}.json`

- Committed directly to main (no PR needed â€” small auto-generated JSON)
- One file per merged PR per day
- Unique filenames per PR (`PR-{number}.json`) â€” no git push race conditions
- **Includes pixel art image URL** â€” generated at PR merge time, reused by Discord posts
- Image generation uses our own API â€” retries 3x with exponential backoff + different seed on 5xx errors

```json
{
  "pr_number": 8117,
  "title": "fix(enter): single-bucket balance deduction",
  "author": "username",
  "url": "https://github.com/pollinations/pollinations/pull/8117",
  "merged_at": "2026-02-09T15:30:00Z",
  "labels": ["bug", "enter"],

  "gist": {
    "category": "bug_fix",
    "user_facing": true,
    "publish_tier": "daily",
    "importance": "major",
    "headline": "One bucket to rule them all",
    "blurb": "The bees fixed a leaky honey jar â€” balance deductions now flow through a single bucket instead of spilling across many.",
    "summary": "Fixed balance deduction to use a single bucket instead of splitting across multiple.",
    "impact": "Users no longer see incorrect balance after API calls.",
    "keywords": ["billing", "balance", "api"],
    "visual_concept": "A bee repairing a cracked piggy bank. The piggy bank represents the single-bucket billing system. Wrench = fix.",
    "image_prompt": "Cozy pixel art scene of a tiny bee fixing a cracked piggy bank with a wrench. Soft lime green glow, chunky 8-bit sprites, warm lighting."
  },

  "image": {
    "url": "https://raw.githubusercontent.com/.../PR-8117.jpg",
    "prompt": "Cozy pixel art scene of a tiny bee fixing a cracked piggy bank..."
  },

  "generated_at": "2026-02-09T15:31:00Z"
}
```

**Key fields:**

| Field | Purpose |
|---|---|
| `publish_tier` | `"none"` / `"discord_only"` / `"daily"` â€” controls which tiers pick up this PR. See `publish_tier` decision logic below. |
| `importance` | `"major"` / `"minor"` â€” AI picks. Binary: headline-worthy or not. |
| `user_facing` | Boolean â€” AI determines if end users would notice this change |
| `headline` | Short creative name for the change (3-8 words). Used in daily/weekly narrative arcs. |
| `blurb` | Whimsical 1-2 sentence description for the website diary. Bee/nature metaphors welcome. |
| `visual_concept` | AI reasoning space â€” identifies project mascots, symbols, and scene concept before writing `image_prompt`. |

**Importance is binary:**

- `"major"` â€” headline-worthy. Users would notice or care. Features, significant bug fixes, new models, pricing changes.
- `"minor"` â€” everything else. Chore, deps, infra, small fixes, internal tooling.

The AI picks based on PR content. The daily summary uses `major` PRs as headline arcs; `minor` PRs get mentioned briefly or grouped. No numeric scores, no formula â€” prominence is implicit in the narrative structure, not serialized as extra fields.

**`publish_tier` decision logic:**

The AI chooses `publish_tier` as part of gist analysis, but hard rules act as guardrails:

```
# Hard rules (override AI choice):
if labels include "deps" or "chore" AND user_facing == false:
    publish_tier = "discord_only"       # forced
if labels include "feature":
    publish_tier = min("daily", AI_choice)  # at least "daily"

# AI decides (with default):
if no labels:
    publish_tier = AI_choice            # default: "daily"
else:
    publish_tier = AI_choice            # default: "daily"

# Valid values: "none", "discord_only", "daily"
# ("weekly" is not a valid tier â€” weekly summary reads gists directly with the same "daily" filter)
```

This means: deps/chore PRs can't sneak into daily summaries, features always make it, and everything else the AI decides with a bias toward inclusion.

### Daily Posts: `social/news/daily/YYYY-MM-DD/`

- `twitter.json` â€” platform post JSON
- `instagram.json` â€” same schema
- `reddit.json` â€” same schema (LinkedIn is weekly-only, no daily file)
- `images/` â€” all generated images

### Weekly: `social/news/weekly/YYYY-MM-DD/`

- `twitter.json` â€” weekly recap tweet (1 image)
- `linkedin.json` â€” weekly recap post (1 image)
- `instagram.json` â€” weekly recap carousel (3 images)
- `reddit.json` â€” weekly Reddit post (1 image)
- `discord.json` â€” weekly Discord digest
- `images/` â€” all generated images
- Generated **Sunday 06:00 UTC**, published **Sunday 18:00 UTC** via cron: Buffer (X, LI, IG) + Reddit API + Discord webhook â€” all 5 platforms. Sunday evening = week wrap-up energy.

---

## Workflows

| Workflow | Trigger |
|---|---|
| `NEWS_pr_gist.yml` | `pull_request_target: closed+merged` â€” per-PR gist + Discord |
| `NEWS_summary.yml` | `cron: 0 6 * * *` â€” Mon-Sat: `generate_daily.py`, Sunday: `generate_weekly.py` |
| `NEWS_publish.yml` | Daily PR merge â†’ `publish_daily.py`; Sunday 18:00 UTC cron â†’ `publish_weekly.py` |

## Scripts

| Script | Purpose |
|---|---|
| `generate_realtime.py` | Per-PR: AI analysis â†’ gist JSON â†’ image gen (source of truth) |
| `publish_realtime.py` | Per-PR: reads gist â†’ AI announcement â†’ Discord webhook post |
| `generate_daily.py` | Daily: read gists â†’ summary + platform posts (X, IG, Reddit) + images |
| `generate_weekly.py` | Weekly: read gists directly (Sunâ†’Sat) â†’ synthesize themes â†’ all 5 platform posts + images |
| `publish_daily.py` | On daily PR merge: Buffer stage (X, IG) + Reddit VPS deployment. LinkedIn = weekly only. |
| `publish_weekly.py` | Sunday 18:00 UTC cron: check if weekly PR merged, then Buffer (X, LI, IG) + Reddit API + Discord webhook |
| `update_highlights.py` | Daily: reads yesterday's gists, AI curates highlights, updates highlights.md + README in single PR |
| `update_readme.py` | Utility functions: `get_top_highlights()`, `update_readme_news_section()` (used by update_highlights.py) |
| `common.py` | Shared utils: prompt loading, brand injection, API calls, gist I/O, retry logic, constants |
| `buffer_publish.py` | Buffer API staging with scheduled delivery |
| `buffer_utils.py` | Buffer GraphQL API helpers |

---

## Error Handling

### Tier 1: `generate_realtime.py` â€” the critical path

The gist is the anchor for everything downstream. The 2 steps run sequentially:

```
Step 1: AI analysis â†’ gist JSON â†’ validate schema â†’ commit to main
  â”œâ”€â”€ Success: proceed to Step 2
  â”œâ”€â”€ Schema validation failure: log warning, commit MINIMAL gist (PR metadata only)
  â”‚   (prevents malformed JSON from reaching downstream tiers)
  â””â”€â”€ AI failure: RETRY up to 3x with exponential backoff
        â””â”€â”€ Still fails: commit a MINIMAL gist (PR metadata only, no AI fields)
            + log error + open GitHub issue tagged "news-pipeline-failure"
            (daily summary sees the PR exists but skips it for narrative)

Step 2: Image generation â†’ update gist with image URL
  â”œâ”€â”€ Success: done (gist fully committed)
  â””â”€â”€ Failure (5xx): RETRY up to 3x with exponential backoff + different seed each attempt
        â””â”€â”€ Still fails: gist.image.url = null, continue without image
            (Discord posts text-only)
```

Discord posting (`publish_realtime.py`) runs as a **separate workflow step** after the gist generator. If Discord posting fails, the gist is already committed (source of truth preserved). Discord is best-effort notification.

### Tier 2: `generate_daily.py`

- If gist directory is **empty** (no PRs merged that day): skip. No PR created, no posts generated. Quiet days are quiet days.
- If gist directory is **missing** (workflow bug): fall back to `get_merged_prs()` from GitHub GraphQL directly. Log a warning.

### Tier 3: `generate_weekly.py`

- Reads gists directly for the week (Sunâ†’Sat). No dependency on daily summaries.
- If gist directory is **empty** for all days (no PRs merged that week): skip. No PR created.

### Re-triggering

All workflows support `workflow_dispatch` for manual re-triggering:
- `NEWS_pr_gist.yml`: accepts `pr_number` input to regenerate a specific gist
- `NEWS_summary.yml`: accepts `date` input (Mon-Sat runs daily, Sunday runs weekly)

---

## Concurrency & Race Conditions

### Simultaneous PR merges

Multiple PRs merging within seconds is the main risk for Tier 1.

**Mitigation:** Each gist writes to a **unique filename** (`PR-{number}.json`). There are no file collisions. The GitHub Contents API commit uses the `sha` parameter for conditional writes â€” if two workflows try to create files in the same directory simultaneously, both succeed because they're writing different files. (Unlike editing the same file, creating new files in a directory doesn't conflict.)

### Daily/weekly generators reading while gists are being written

The daily summary runs at 06:00 UTC. A PR merged at 05:59 UTC might have its gist committed at 06:01.

**Mitigation:** The daily summary selects gists by **`merged_at` timestamp**, not by file commit time. It reads all gists where `merged_at` falls on the target date, regardless of when the file appeared on main. No sleep needed. If a gist is committed *after* the daily summary already ran (e.g., slow image gen), it gets picked up by the next day's summary or by the weekly fallback.

---

## Key Design Decisions

1. **PR-time analysis is the anchor** â€” intent is frozen while context is freshest. Eliminates platform drift ("same PR, different story"). Reduces AI cost and variance.

2. **Gists stored as repo files, not GitHub Gist API** â€” auditable, diffable, reviewable. No extra auth surface. Fits existing repo-as-CMS pattern.

3. **Gists committed directly to main (no PR)** â€” small auto-generated metadata. Unique filenames prevent collisions. The daily summary PR is where human review happens.

4. **`publish_tier` field gates what reaches each tier** â€” non-user-facing PRs default to `discord_only`, preventing "busy weeks" from reading like spam. Daily/weekly layers only consume PRs tagged `daily` or higher.

5. **Importance is binary** â€” `major/minor` chosen by AI. Headline-worthy or not. Prominence is implicit in narrative structure, not serialized as extra fields.

6. **2 sequential steps in Tier 1** â€” gist commit, then image gen. Gist (the anchor) commits first; image gen retries 3x on 5xx (different seed each time). Discord posting is a separate workflow step (`publish_realtime.py`) â€” best-effort, decoupled from the source of truth.

7. **Single daily PR instead of 3** â€” one PR contains twitter.json + instagram.json + reddit.json + images. LinkedIn is weekly-only. Humans review narrative, not fragments.

8. **Daily summary clusters related PRs into 3-5 story arcs** â€” 5 PRs about the same subsystem become one narrative beat. Editorial quality, not a changelog.

9. **Three independent image families** â€” see Image Generation Strategy section below.

10. **Highlights + README in the daily PR** â€” `generate_daily.py` curates yesterday's gists into `highlights.md` and updates the README "Latest News" section, all within the same daily PR. No separate workflow.

11. **Weekly delivery at Sunday 18:00 UTC** â€” Sunday evening "week wrap-up" energy. All 5 platforms at once.

12. **No fallback content for zero-PR days** â€” if no PRs merged, the daily workflow skips entirely. No PR created, no posts. Quiet days are quiet days.

13. **Website diary reads from gists + summary** â€” no separate diary generation step. Gists include `headline` and `blurb` fields; daily/weekly summaries include `mood`. The website can render a diary view directly from these sources.

14. **Weekly reads gists directly, independent of dailies** â€” the weekly summary reads the week's gists (Sunâ†’Sat) and synthesizes themes into a bigger narrative ("this week we shipped X, fixed Y, started Z"). This eliminates the dependency on daily summaries being generated first, ensuring no PRs are missed.

---

## Image Generation Strategy

There are **3 independent families of images**. Each tier generates its own images with its own prompts and style.

| Family | Generated by | When | Style | Count | Used by |
|---|---|---|---|---|---|
| **Per-PR pixel art** | `generate_realtime.py` | Tier 1 (on PR merge) | 8-bit pixel art | 1 per PR | Discord post, website diary |
| **Daily platform images** | `generate_daily.py` | Tier 2 (06:00 UTC) | Brand pixel art (from `brand/visual.md`) | 1 Twitter + 3 Instagram + 1 Reddit = **5 per day** | Twitter, Instagram, Reddit daily posts (LinkedIn = weekly only) |
| **Weekly platform images** | `generate_weekly.py` | Tier 3 (Sunday 06:00 UTC) | Brand pixel art (from `brand/visual.md`) | 1 Twitter + 1 LinkedIn + 3 Instagram + 1 Reddit + 1 Discord = **7 per week** | Twitter, LinkedIn, Instagram, Reddit, Discord weekly posts |

**Key points:**

- **Daily and weekly images are freshly generated** from the daily narrative / weekly summary. They are NOT the per-PR pixel art images. The AI creates images that illustrate the aggregated story, not individual PRs.

---

## Cost Estimate (per day, assuming 5 PRs merged)

| Step | AI calls | Image gens |
|---|---|---|
| PR gists (5x) | 5 | 5 |
| Daily summary (1x) | 2 | 5 (1 twitter + 3 instagram + 1 reddit) |
| **Total** | **7** | **10** |

Weekly adds ~6 AI calls + ~7 image gens on Sundays.

AI calls scale as N+1 (N per-PR gists + 1 daily summary), not NÃ—platforms. Image count stays similar across any architecture (same images needed).

---

## Verification

1. **Tier 1 â€” happy path**: Merge a test PR â†’ verify gist JSON committed to `social/news/gists/` + image generated + Discord post sent (separate step)
2. **Tier 1 â€” AI failure**: Mock AI to fail â†’ verify minimal gist (metadata only) committed + GitHub issue opened
3. **Tier 2 â€” happy path**: Manually trigger daily workflow â†’ verify single PR with all platform posts + images
4. **Tier 2 â€” zero PRs**: Run daily workflow on a day with 0 gists â†’ verify workflow exits cleanly with no PR created
5. **Tier 3 â€” happy path**: Manually trigger weekly workflow â†’ verify PR with all 5 platform posts + images
6. **Daily publish**: Merge daily PR â†’ verify Buffer stages X/IG (no LinkedIn â€” weekly only)
7. **Weekly publish**: Merge weekly PR before Sunday 18:00 UTC â†’ verify Sunday 18:00 cron publishes all 5 platforms (Buffer X/LI/IG + Reddit API + Discord webhook)
7b. **Weekly publish â€” PR not merged**: Don't merge weekly PR â†’ verify Sunday 18:00 cron skips cleanly
8. **Publish tier gating**: Merge a non-user-facing PR â†’ verify `publish_tier: discord_only` â†’ verify absent from daily summary
9. **Clustering**: Day with 5+ related PRs â†’ verify daily summary groups them into narrative arcs (not a flat list)
10. **Concurrent merges**: Merge 3 PRs within 30 seconds â†’ verify all 3 gists committed without conflicts
11. **Late gist commit**: Merge a PR on day N, delay gist commit to day N+1 â†’ verify the day N daily summary (if already run) misses it, and the day N+1 summary picks it up via `merged_at` timestamp

---

## Critical Files

| File | Role |
|---|---|
| `social/scripts/common.py` | Shared utilities: prompt loading, brand injection, API calls, gist I/O, retry logic, constants |
| `social/scripts/buffer_publish.py` | Buffer API staging with scheduled delivery |
| `social/buffer-schedule.yml` | Delivery schedule for all platforms |

## Prompts

All prompts live in `social/prompts/`:

```
social/prompts/
  brand/                       # Brand components (auto-injected via placeholders)
    about.md                   # Company description       â†’ {about}
    visual.md                  # Pixel art style guide     â†’ {visual_style}
    bee.md                     # Bee mascot description    â†’ {bee_character}
    links.md                   # Official links            â†’ {links}

  tone/                        # Platform voices (system prompts)
    twitter.md                 # Twitter/X voice + image adaptation
    linkedin.md                # LinkedIn voice + image adaptation
    instagram.md               # Instagram voice + image adaptation
    reddit.md                  # Reddit voice + image adaptation
    discord.md                 # Discord voice + image adaptation

  gist.md                      # Tier 1: Analyze PR â†’ gist JSON + image prompt
  daily.md                     # Tier 2: Cluster gists into 3-5 narrative arcs
  weekly.md                    # Tier 3: Synthesize weekly recap from gists
  highlights.md                # Highlights curation for GitHub + README
  format.md                    # Output format specs (JSON schemas per platform)
```

### Brand Injection

`common.py` automatically replaces placeholders in any loaded prompt:

| Placeholder | Source |
|---|---|
| `{about}` | `brand/about.md` |
| `{visual_style}` | `brand/visual.md` |
| `{bee_character}` | `brand/bee.md` |
| `{links}` | `brand/links.md` |

### Prompt Composition Pattern

Every platform post is generated from **three layers** combined:

1. **Brand identity** (`brand/*.md`) â€” injected automatically via placeholders. Defines who we are, visual style, bee mascot.
2. **Platform voice** (`tone/<platform>.md`) â€” system prompt. Defines tone, length, formatting rules, image adaptation for a specific destination.
3. **Output format** (`format.md`) â€” user prompt. Defines the JSON schema and content constraints for each platform.

The AI call structure: `system_prompt = load_prompt("tone/{platform}")` (with brand auto-injected) + `user_prompt = summary_data + load_format("{platform}")`.

This allows reusing the same voice across cadences (daily, weekly) and the same format across content types.
